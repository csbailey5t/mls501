---
title: "Social Media and Covid-19 Twitter Data Activity"
author: "Scott Bailey"
date: "5/28/2020"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro

Building on our conversations on social media, analysis, and surveillance, let's actually do a bit of work with a random subset of Twitter data about Covid-19. Here's the full dataset: https://zenodo.org/record/3842180. The random sample of 50k tweets came from the data as it was on April 13, 2020. 

Let's start by importing different libraries into our R session that will help with different parts of our exploration and analysis. 

```{r libraries}
library(tidyverse, warn.conflicts = FALSE)
library(leaflet, warn.conflicts = FALSE)
library(stringr)
library(tidytext)
library(wordcloud)
```

We then need to read in our Twitter data, which is stored in a csv file called `covid_50k.csv`. To see a bit of what we have we can view a random sample of tweets. 

```{r dataimports}
tweets <- read_csv("covid_50k.csv")
sample_n(tweets, 10)
```

Hashtags are an important piece of Twitter for indicating subject, but also memes, slogans, political approaches, etc. Let's figure out what the top hashtags from our dataset. Since the `hashtags` column contains multiple hashtags, we'll need to split that apart into single words before we can count them. 

```{r allhashtags}
tweets %>%
  unnest_tokens(hashtag, hashtags, token = "words", to_lower = TRUE) %>%
  count(hashtag) %>%
  arrange(-n)
```

What do you think the `NA` means in the counts above? 

We can build on the same code, and instead of arranging it to see the most frequent hashtags, we can build a wordcloud from them. 

```{r wordcloud}
tweets %>%
  unnest_tokens(hashtag, hashtags, token = "words", to_lower = TRUE) %>%
  count(hashtag) %>%
  with(wordcloud(hashtag, n, random.order = FALSE, max.words = 50))
```
We can start to subset our data based on hashtags. Let's try to find just the tweets that have hashtags that contain the word `confinement`. 

```{r hashtagfilter}
tweets %>%
  filter(str_detect(hashtags, "confinement"))
```

We can specify filter conditions where we look for the presence of any of multiple terms. 

```{r hashtagfiltermultiple}
# Find tweets that contain one from a set of hashtags; the pipe here is an "or" operator
tweets %>%
  filter(str_detect(hashtags, "trump") | str_detect(hashtags, "biden"))
```

What if we wanted to understand the most liked or "favorited" tweets in this sample of tweets? We can arrange the tweets in descending order according to the column `favorite_count`. 

```{r mostlikedtweets}
tweets %>%
  arrange(-favorite_count)
```

Some of the tweets do contain coordinate data that indicates where the tweet was written. We need to do a bit of work to break apart the longitude and latitude, but once we've done that, we can send the data to the `leaflet` library for mapping. Once you've run the code, try clicking on the markers on the map. 

```{r}
# Notice that when we drop all the tweets where we don't have coordinate information, we only have 52 of 50,000 left.
tweets %>%
  drop_na(coordinates) %>%
  separate(coordinates, c("lon", "lat"), sep = ",", convert = T) %>%
  leaflet() %>%
    addTiles() %>%  # Add default OpenStreetMap map tiles
    addMarkers(~as.numeric(lon), ~as.numeric(lat), popup = ~as.character(text),
             labelOptions = labelOptions(textsize = "20px"))
```

We've just been exploring so far, but let's think about some of the applications we've talked about and how we might go about it ourselves. 

Let's start by thinking like someone who might want to see if people are tweeting about feeling ill, and find any tweets that mention a set of related words. We'll then pull all the screennames and some associated info for those tweets. We'll do this without narrowing further, but there are methods in natural language processing that would let you find just tweets where a person seems to talking about themselves or another person coughing, sneezing, breath hard, feeling ill, etc. 

```{r}
tweets %>%
  filter(str_detect(text, "cough") | str_detect(text, "sick") | str_detect(text, "breathing")) %>%
  select(user_name, user_location, user_description, text)
```

Let's take a step back though - this same process of filtering tweets by content and looking at user information, including locations, is exactly the same basis as could be used to work on predicting disease spread, or doing early detection of plant or animal disease. 
